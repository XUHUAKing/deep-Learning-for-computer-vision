{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# World Navigation with DQN\n",
    "\n",
    "In this exercise you will play a world navigation game with Deep Q-Networks. The agent learn to solve a navigation task in a basic grid world. It will be built upon the simple one layer Q-network you created in Exercise 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to transform an ordinary Q-Network into a DQN you will be making the following improvements:\n",
    "1. Going from a single-layer network to a multi-layer convolutional network.\n",
    "2. Implementing Experience Replay, which will allow our network to train itself using stored memories from it's experience.\n",
    "3. Utilizing a second \"target\" network, which we will use to compute target Q-values during our updates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will also implement two simple additional improvements to the DQN architecture, Double DQN and Dueling DQN, that allow for improved performance, stability, and faster training time. In the end you will have a network that can tackle a number of challenging Atari games, and we will demonstrate how to train the DQN to learn a basic navigation task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the game environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAADNRJREFUeJzt3X/oXfV9x/Hna4nW1m41URcyo/tmVJQwMLrgFMvotG7W\nFd0fRZQyyhD8p9t0LbS6/SGF/dHCaOsfoyC1nQznj1pdQyh2LrWM/ZMaf6zVRGu0sSaoiZ3OzsG2\ntO/9cU/Yt1ni93zzvb+On+cDvnzvOfdezud4eX3PuceTzytVhaS2/NKsByBp+gy+1CCDLzXI4EsN\nMvhSgwy+1CCDLzVoRcFPckWSZ5PsSXLzuAYlabJyvDfwJFkF/BC4HNgHPApcV1W7xjc8SZOwegXv\nvRDYU1UvACS5B7gaOGbwTzvttFpYWFjBJiW9nb179/Laa69lqdetJPhnAC8tWt4H/PbbvWFhYYGd\nO3euYJOS3s6WLVt6vW7iF/eS3JBkZ5KdBw8enPTmJPWwkuDvB85ctLyhW/cLqur2qtpSVVtOP/30\nFWxO0risJPiPAmcn2ZjkROBaYOt4hiVpko77O35VHUryJ8C3gVXAV6vq6bGNTNLErOTiHlX1LeBb\nYxqLpCnxzj2pQQZfapDBlxpk8KUGGXypQQZfapDBlxpk8KUGGXypQQZfapDBlxpk8KUGGXypQQZf\napDBlxpk8KUGGXypQUsGP8lXkxxI8tSidWuTPJzkue73mskOU9I49Tni/y1wxRHrbga2V9XZwPZu\nWdJALBn8qvpn4N+OWH01cGf3+E7gD8c8LkkTdLzf8ddV1cvd41eAdWMaj6QpWPHFvRq1bh6zedMm\nHWn+HG/wX02yHqD7feBYL7RJR5o/xxv8rcDHu8cfB745nuFImoYlCzWS3A18EDgtyT7gVuBzwH1J\nrgdeBK6Z5CDHIVmyOXhijvk9aApmt9edRnd+9A14fi0Z/Kq67hhPXTbmsUiaEu/ckxpk8KUGGXyp\nQQZfapDBlxpk8KUGGXypQQZfapDBlxpk8KUGGXypQQZfapDBlxpk8KUGGXypQQZfapDBlxrUp0nn\nzCSPJNmV5OkkN3brbdORBqrPEf8Q8Kmq2gRcBHwiySZs05EGq0+TzstV9Xj3+KfAbuAMbNORBmtZ\n3/GTLADnAzvo2aZjoYY0f3oHP8l7gW8AN1XVm4ufe7s2HQs1pPnTK/hJTmAU+ruq6oFude82HUnz\npc9V/QB3ALur6guLnrJNRxqoJQs1gEuAPwJ+kOTJbt1fMMA2HUkjfZp0/oVjlxHZpiMNkHfuSQ0y\n+FKDDL7UoD4X97RCM6+qnqVZ7vx8N1XPlEd8qUEGX2qQwZcaZPClBhl8qUEGX2qQwZcaZPClBhl8\nqUEGX2qQwZcaZPClBhl8qUF95tw7Kcn3kvxr16Tz2W79xiQ7kuxJcm+SEyc/XEnj0OeI/1/ApVV1\nHrAZuCLJRcDngS9W1fuB14HrJzdMSePUp0mnquo/usUTup8CLgXu79bbpCMNSN959Vd1M+weAB4G\nngfeqKpD3Uv2MarVOtp7bdKR5kyv4FfVz6pqM7ABuBA4t+8GbNKR5s+yrupX1RvAI8DFwClJDk/d\ntQHYP+axSZqQPlf1T09ySvf43cDljBpzHwE+2r3MJh1pQPpMtrkeuDPJKkZ/KO6rqm1JdgH3JPkr\n4AlGNVuSBqBPk873GVVjH7n+BUbf9yUNjHfuSQ0y+FKDDL7UIIMvNcjgSw0y+FKDDL7UIIMvNcia\n7GmYZV1zyx3dVnQfk0d8qUEGX2qQwZcaZPClBhl8qUEGX2qQwZcaZPClBvUOfjfF9hNJtnXLNulI\nA7WcI/6NjCbZPMwmHWmg+hZqbAD+APhKtxxs0pEGq+8R/0vAp4Gfd8unYpOONFh95tX/CHCgqh47\nng3YpCPNnz7/Ou8S4KokVwInAb8C3EbXpNMd9W3SkQakT1vuLVW1oaoWgGuB71TVx7BJRxqslfx/\n/M8An0yyh9F3fpt0pIFY1kQcVfVd4LvdY5t0pIHyzj2pQQZfapDBlxpk8KUGGXypQQZfapDBlxpk\n8KUGGXypQQZfapDBlxpk8KUGGXypQQZfatCy/lmujlPLHfWaSx7xpQb1OuIn2Qv8FPgZcKiqtiRZ\nC9wLLAB7gWuq6vXJDFPSOC3niP+7VbW5qrZ0yzcD26vqbGB7tyxpAFZyqn81oyINsFBDGpS+wS/g\nH5M8luSGbt26qnq5e/wKsG7so5M0EX2v6n+gqvYn+VXg4STPLH6yqipJHe2N3R+KGwDOOuusFQ1W\n0nj0OuJX1f7u9wHgQUaz676aZD1A9/vAMd5rk440Z/pUaJ2c5JcPPwZ+D3gK2MqoSAMs1JAGpc+p\n/jrgwVFBLquBv6+qh5I8CtyX5HrgReCayQ1T0jgtGfyuOOO8o6z/CXDZJAYlabK8c09qkMGXGmTw\npQYZfKlBBl9qkMGXGmTwpQYZfKlBBl9qkMGXGmTwpQYZfKlBBl9qkMGXGmTwpQYZfKlBBl9qUK/g\nJzklyf1JnkmyO8nFSdYmeTjJc93vNZMerKTx6HvEvw14qKrOZTQN125s0pEGq88su+8Dfge4A6Cq\n/ruq3sAmHWmw+syyuxE4CHwtyXnAY8CNDK5J56h9H1Pa8ux6sptu6J7dRz73+pzqrwYuAL5cVecD\nb3HEaX1VFcf4z5zkhiQ7k+w8ePDgSscraQz6BH8fsK+qdnTL9zP6Q2CTjjRQSwa/ql4BXkpyTrfq\nMmAXNulIg9W3NPNPgbuSnAi8APwxoz8aNulIA9Qr+FX1JLDlKE/ZpCMNkHfuSQ0y+FKDDL7UIIMv\nNcjgSw0y+FKDDL7UIIMvNcjgSw0y+FKDDL7UIIMvNcjgSw0y+FKDDL7UIIMvNcjgSw3qM6/+OUme\nXPTzZpKbbNKRhqvPZJvPVtXmqtoM/Bbwn8CD2KQjDdZyT/UvA56vqhexSUcarOUG/1rg7u7xwJp0\nJB3WO/jd1NpXAV8/8jmbdKRhWc4R/8PA41X1ardsk440UMsJ/nX832k+2KQjDVav4Cc5GbgceGDR\n6s8Blyd5DvhQtyxpAPo26bwFnHrEup8woCadmmll8iwruqX/zzv3pAYZfKlBBl9qkMGXGmTwpQYZ\nfKlBBl9qkMGXGmTwpQYZfKlBBl9qkMGXGmTwpQYZfKlBBl9qkMGXGmTwpQb1nXrrz5M8neSpJHcn\nOSnJxiQ7kuxJcm83C6+kAehToXUG8GfAlqr6TWAVo/n1Pw98sareD7wOXD/JgUoan76n+quBdydZ\nDbwHeBm4FLi/e94mHWlA+nTn7Qf+Gvgxo8D/O/AY8EZVHepetg84Y1KDlDRefU711zDqydsI/Bpw\nMnBF3w3YpCPNnz6n+h8CflRVB6vqfxjNrX8JcEp36g+wAdh/tDfbpCPNnz7B/zFwUZL3JAmjufR3\nAY8AH+1eY5OONCB9vuPvYHQR73HgB917bgc+A3wyyR5GZRt3THCcksaob5POrcCtR6x+Abhw7COS\nNHHeuSc1yOBLDTL4UoMMvtSg1BT7o5McBN4CXpvaRifvNNyfefVO2hfotz+/XlVL3jAz1eADJNlZ\nVVumutEJcn/m1ztpX2C8++OpvtQggy81aBbBv30G25wk92d+vZP2Bca4P1P/ji9p9jzVlxo01eAn\nuSLJs908fTdPc9srleTMJI8k2dXNP3hjt35tkoeTPNf9XjPrsS5HklVJnkiyrVse7FyKSU5Jcn+S\nZ5LsTnLxkD+fSc51ObXgJ1kF/A3wYWATcF2STdPa/hgcAj5VVZuAi4BPdOO/GdheVWcD27vlIbkR\n2L1oechzKd4GPFRV5wLnMdqvQX4+E5/rsqqm8gNcDHx70fItwC3T2v4E9uebwOXAs8D6bt164NlZ\nj20Z+7CBURguBbYBYXSDyOqjfWbz/AO8D/gR3XWrResH+fkwmsruJWAto39Fuw34/XF9PtM81T+8\nI4cNdp6+JAvA+cAOYF1Vvdw99QqwbkbDOh5fAj4N/LxbPpXhzqW4ETgIfK376vKVJCcz0M+nJjzX\npRf3linJe4FvADdV1ZuLn6vRn+FB/G+SJB8BDlTVY7Mey5isBi4AvlxV5zO6NfwXTusH9vmsaK7L\npUwz+PuBMxctH3OevnmV5ARGob+rqh7oVr+aZH33/HrgwKzGt0yXAFcl2Qvcw+h0/zZ6zqU4h/YB\n+2o0YxSMZo26gOF+Piua63Ip0wz+o8DZ3VXJExldqNg6xe2vSDff4B3A7qr6wqKntjKacxAGNPdg\nVd1SVRuqaoHRZ/GdqvoYA51LsapeAV5Kck636vDckIP8fJj0XJdTvmBxJfBD4HngL2d9AWWZY/8A\no9PE7wNPdj9XMvpevB14DvgnYO2sx3oc+/ZBYFv3+DeA7wF7gK8D75r1+JaxH5uBnd1n9A/AmiF/\nPsBngWeAp4C/A941rs/HO/ekBnlxT2qQwZcaZPClBhl8qUEGX2qQwZcaZPClBhl8qUH/C5Jc7HOD\nRvnlAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c02a3b68d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from gridworld import gameEnv\n",
    "\n",
    "env = gameEnv(partial=False,size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is an example of a starting environment in our simple game. The game environment outputs 84x84x3 color images, and uses function calls as similar to the OpenAI gym as possible. The agent controls the blue square, and can move up, down, left, or right. The goal is to move to the green square (for +1 reward) and avoid the red square (for -1 reward). \n",
    "\n",
    "At the start of each episode all squares are randomly placed within a 5x5 grid-world. The agent has 50 steps to achieve as large a reward as possible. Because they are randomly positioned, the agent needs to do more than simply learn a fixed path, as was the case in the FrozenLake environment from Exercise 1. Instead the agent must learn a notion of spatial relationships between the blocks. \n",
    "\n",
    "*Feel free to adjust the size of the gridworld (default 5). Making it smaller provides an easier task for our DQN agent, while making the world larger increases the challenge.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addition 1: Convolutional Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first major addition to make DQNs work is to use convolutional layers to set up the networks. We are now familiar with convolutional layers after assignment `1-3`. For more information, see the [Tensorflow documentation](https://www.tensorflow.org/api_docs/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addition 2: Experience Replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second major addition to make DQNs work is Experience Replay. The basic idea is that by storing an agentâ€™s experiences, and then randomly drawing batches of them to train the network, we can more robustly learn to perform well in the task. By keeping the experiences we draw random, we prevent the network from only learning about what it is immediately doing in the environment, and allow it to learn from a more varied array of past experiences. \n",
    "\n",
    "Each of these experiences are stored as a tuple of *(state,action,reward,next state)*. The Experience Replay buffer stores a fixed number of recent memories, and as new ones come in, old ones are removed. When the time comes to train, we simply draw a uniform batch of random memories from the buffer, and train our network with them. \n",
    "\n",
    "For our DQN, we build a simple class that allows us to store experies and sample them randomly to train the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class experience_buffer():\n",
    "    def __init__(self, buffer_size = 50000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "    \n",
    "    def add(self,experience):\n",
    "        if len(self.buffer) + len(experience) >= self.buffer_size:\n",
    "            self.buffer[0:(len(experience)+len(self.buffer))-self.buffer_size] = []\n",
    "        self.buffer.extend(experience)\n",
    "            \n",
    "    def sample(self,size):\n",
    "        return np.reshape(np.array(random.sample(self.buffer,size)),[size,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple function to resize our game frames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def processState(states):\n",
    "    return np.reshape(states,[21168]) # 84 x 84 x 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addition 3: Separate Target Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third major addition to the DQN that makes it unique is the utilization of a second network during the training procedure. This second network is used to generate the target-Q values that will be used to compute the loss for every action during training. Why not use just use one network for both estimations? The issue is that at every step of training, the Q-networkâ€™s values shift, and if we are using a constantly shifting set of values to adjust our network values, then the value estimations can easily spiral out of control. The network can become destabilized by falling into feedback loops between the target and estimated Q-values. In order to mitigate that risk, the target networkâ€™s weights are fixed, and only periodically or slowly updated to the primary Q-networks values. In this way training can proceed in a more stable manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions allow us to update the parameters of our target network with those of the primary network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def updateTargetGraph(tfVars,tau):\n",
    "    total_vars = len(tfVars)\n",
    "    op_holder = []\n",
    "    for idx,var in enumerate(tfVars[0:total_vars//2]):\n",
    "        op_holder.append(tfVars[idx+total_vars//2].assign((var.value()*tau) + ((1-tau)*tfVars[idx+total_vars//2].value())))\n",
    "    return op_holder\n",
    "\n",
    "def updateTarget(op_holder,sess):\n",
    "    for op in op_holder:\n",
    "        sess.run(op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the additions above, we have everything we need to replicate the DQN.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dueling DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to explain the reasoning behind the architecture changes that Dueling DQN makes, we need to first explain some a few additional reinforcement learning terms. The Q-values that we have been discussing so far correspond to how good it is to take a certain action given a certain state. This can be written as `Q(s,a)`. This action given state can actually be decomposed into two more fundamental notions of value. The first is the value function `V(s)`, which says simple how good it is to be in any given state. The second is the advantage function `A(a)`, which tells how much better taking a certain action would be compared to the others. We can then think of `Q` as being the combination of `V` and `A`. More formally:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Q(s,a) =V(s) + A(a) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of Dueling DQN is to have a network that separately computes the advantage and value functions, and combines them back into a single Q-function only at the final layer. It may seem somewhat pointless to do this at first glance. Why decompose a function that we will just put back together? The key to realizing the benefit is to appreciate that our reinforcement learning agent may not need to care about both value and advantage at any given time. We can achieve more robust estimates of state value by decoupling it from the necessity of being attached to specific actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the network itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Qnetwork():\n",
    "    def __init__(self,h_size):\n",
    "        #The network recieves a frame from the game, flattened into an array.\n",
    "        #It then resizes it and processes it through four convolutional layers.\n",
    "        #We use slim.conv2d to set up our network \n",
    "        self.scalarInput =  tf.placeholder(shape=[None,21168],dtype=tf.float32)\n",
    "        self.imageIn = tf.reshape(self.scalarInput,shape=[-1,84,84,3])\n",
    "        self.conv1 = slim.conv2d( \\\n",
    "            inputs=self.imageIn,num_outputs=32,kernel_size=[8,8],stride=[4,4],padding='VALID', biases_initializer=None)\n",
    "        self.conv2 = slim.conv2d( \\\n",
    "            inputs=self.conv1,num_outputs=64,kernel_size=[4,4],stride=[2,2],padding='VALID', biases_initializer=None)\n",
    "        self.conv3 = slim.conv2d( \\\n",
    "            inputs=self.conv2,num_outputs=64,kernel_size=[3,3],stride=[1,1],padding='VALID', biases_initializer=None)\n",
    "        self.conv4 = slim.conv2d( \\\n",
    "            inputs=self.conv3,num_outputs=h_size,kernel_size=[7,7],stride=[1,1],padding='VALID', biases_initializer=None)\n",
    "        #size of conv4 = Nx1x1xh_size\n",
    "        ################################################################################\n",
    "        # TODO: Implement Dueling DQN                                                  #\n",
    "        # We take the output from the final convolutional layer i.e. self.conv4 and    #\n",
    "        # split it into separate advantage and value streams.                          #\n",
    "        # Outout: self.Advantage, self.Value                                           #\n",
    "        # Hint: Refer to Fig.1 in [Dueling DQN](https://arxiv.org/pdf/1511.06581.pdf)  #\n",
    "        #       In implementation, use tf.split to split into two branches. You may    #\n",
    "        #       use xavier_initializer for initializing the two additional linear      #\n",
    "        #       layers.                                                                # \n",
    "        ################################################################################\n",
    "        self.StreamAC, self.StreamVC = tf.split(self.conv4, num_or_size_splits=2, axis=3)\n",
    "        self.StreamA = slim.flatten(self.StreamAC)# N*...\n",
    "        self.StreamV = slim.flatten(self.StreamVC)# N*... -> this Qnetwork calculate only output 4 Q-values!! for one state and 4 actions\n",
    "        \n",
    "        initializer = tf.contrib.layers.xavier_initializer()\n",
    "        #self.AW = tf.Variable(initializer([h_size//2,env.actions]))\n",
    "        #self.VW = tf.Variable(initializer([h_size//2,1]))\n",
    "        \n",
    "        self.AW = tf.Variable(initializer([self.StreamA.get_shape().as_list()[1],env.actions]))#\n",
    "        self.VW = tf.Variable(initializer([self.StreamV.get_shape().as_list()[1],1]))#\n",
    "        \n",
    "        self.Advantage = tf.matmul(self.StreamA, self.AW)#Nx4\n",
    "        self.Value = tf.matmul(self.StreamV, self.VW)#Nx1\n",
    "    \n",
    "        ################################################################################\n",
    "        #                                 END OF YOUR CODE                             #\n",
    "        ################################################################################\n",
    "        \n",
    "        #Then combine them together to get our final Q-values. \n",
    "        #Please refer to Equation (9) in [Dueling DQN](https://arxiv.org/pdf/1511.06581.pdf)\n",
    "        self.Qout = self.Value + tf.subtract(self.Advantage,tf.reduce_mean(self.Advantage,axis=1,keep_dims=True))#Nx4\n",
    "        self.predict = tf.argmax(self.Qout,1) #Nx1 action!!, every batch-> every state its optimal action, NOT USED FOR TRAINING!!\n",
    "        \n",
    "        #Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.\n",
    "        self.targetQ = tf.placeholder(shape=[None],dtype=tf.float32)#Nx1 value for Q*(s,a) by bellman\n",
    "        self.actions = tf.placeholder(shape=[None],dtype=tf.int32)#Nx1\n",
    "        self.actions_onehot = tf.one_hot(self.actions,env.actions,dtype=tf.float32)#Nx4\n",
    "        \n",
    "        ################################################################################\n",
    "        # TODO:                                                                        #\n",
    "        # Obtain the loss (self.loss) by taking the sum of squares difference          #\n",
    "        # between the target and prediction Q values.                                  #\n",
    "        ################################################################################\n",
    "        #Qout is different from predictQ, Qout is Nx4, contain all actions' Q-value for every batch N\n",
    "        #while predictQ only picks the predicted Q-value for specific action because we only focus on Q(s,a)\n",
    "        self.predictQ = tf.reduce_sum(tf.multiply(self.Qout, self.actions_onehot),axis = 1)#Nx1 for Q*(s,a,theta) by NN\n",
    "        square_diff = tf.square(self.predictQ - self.targetQ)\n",
    "        self.loss = tf.reduce_mean(square_diff)\n",
    "        ################################################################################\n",
    "        #                                 END OF YOUR CODE                             #\n",
    "        ################################################################################\n",
    "        \n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        self.updateModel = self.trainer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting all the training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32 #How many experiences to use for each training step.\n",
    "update_freq = 4 #How often to perform a training step.\n",
    "y = .99 #Discount factor on the target Q-values\n",
    "startE = 1 #Starting chance of random action\n",
    "endE = 0.1 #Final chance of random action\n",
    "annealing_steps = 10000. #How many steps of training to reduce startE to endE.\n",
    "num_episodes = 5000 #How many episodes of game environment to train network with.\n",
    "pre_train_steps = 10000 #How many steps of random actions before training begins.\n",
    "max_epLength = 50 #The max allowed length of our episode.\n",
    "load_model = False #Whether to load a saved model.\n",
    "path = \"./dqn\" #The path to save our model to.\n",
    "h_size = 512 #The size of the final convolutional layer before splitting it into Advantage and Value streams.\n",
    "tau = 0.001 #Rate to update target network toward primary network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main intuition behind Double DQN is that the regular DQN often overestimates the Q-values of the potential actions to take in a given state. While this would be fine if all actions were always overestimates equally, there was reason to believe this wasnâ€™t the case. You can easily imagine that if certain suboptimal actions regularly were given higher Q-values than optimal actions, the agent would have a hard time ever learning the ideal policy. In order to correct for this, the authors of DDQN paper propose a simple trick: instead of taking the max over Q-values when computing the target-Q value for our training step, we use our primary network to chose an action, and our target network to generate the target Q-value for that action. By decoupling the action choice from the target Q-value generation, we are able to substantially reduce the overestimation, and train faster and more reliably. Below is the new DDQN equation for updating the target value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Q \\text{-}target = r + Î³Q(s',\\arg \\max(Q(sâ€™,a,\\theta),\\theta')) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model\n",
      "Episode 9 reward: 2.5\n",
      "Episode 19 reward: 2.4\n",
      "Episode 29 reward: 2.7\n",
      "Episode 39 reward: 2.6\n",
      "Episode 49 reward: 2.4\n",
      "Episode 59 reward: 0.2\n",
      "Episode 69 reward: 2.4\n",
      "Episode 79 reward: 1.3\n",
      "Episode 89 reward: 1.9\n",
      "Episode 99 reward: 1.8\n",
      "Episode 109 reward: 1.8\n",
      "Episode 119 reward: 2.0\n",
      "Episode 129 reward: 3.5\n",
      "Episode 139 reward: 2.2\n",
      "Episode 149 reward: 2.3\n",
      "Episode 159 reward: 2.1\n",
      "Episode 169 reward: 3.4\n",
      "Episode 179 reward: 4.1\n",
      "Episode 189 reward: 3.1\n",
      "Episode 199 reward: 1.3\n",
      "Episode 209 reward: 1.7\n",
      "Episode 219 reward: 2.2\n",
      "Episode 229 reward: 1.1\n",
      "Episode 239 reward: 0.6\n",
      "Episode 249 reward: 1.6\n",
      "Episode 259 reward: 3.2\n",
      "Episode 269 reward: 1.8\n",
      "Episode 279 reward: 1.6\n",
      "Episode 289 reward: 4.1\n",
      "Episode 299 reward: 3.5\n",
      "Episode 309 reward: 3.0\n",
      "Episode 319 reward: 5.3\n",
      "Episode 329 reward: 4.4\n",
      "Episode 339 reward: 1.8\n",
      "Episode 349 reward: 4.8\n",
      "Episode 359 reward: 3.8\n",
      "Episode 369 reward: 2.7\n",
      "Episode 379 reward: 5.8\n",
      "Episode 389 reward: 4.5\n",
      "Episode 399 reward: 3.5\n",
      "Episode 409 reward: 5.9\n",
      "Episode 419 reward: 4.2\n",
      "Episode 429 reward: 6.3\n",
      "Episode 439 reward: 4.9\n",
      "Episode 449 reward: 7.3\n",
      "Episode 459 reward: 6.9\n",
      "Episode 469 reward: 8.1\n",
      "Episode 479 reward: 6.0\n",
      "Episode 489 reward: 8.7\n",
      "Episode 499 reward: 9.8\n",
      "Episode 509 reward: 10.5\n",
      "Episode 519 reward: 10.4\n",
      "Episode 529 reward: 14.9\n",
      "Episode 539 reward: 10.3\n",
      "Episode 549 reward: 15.2\n",
      "Episode 559 reward: 13.1\n",
      "Episode 569 reward: 15.8\n",
      "Episode 579 reward: 20.2\n",
      "Episode 589 reward: 14.3\n",
      "Episode 599 reward: 13.5\n",
      "Episode 609 reward: 14.4\n",
      "Episode 619 reward: 18.7\n",
      "Episode 629 reward: 17.4\n",
      "Episode 639 reward: 17.4\n",
      "Episode 649 reward: 16.3\n",
      "Episode 659 reward: 19.6\n",
      "Episode 669 reward: 14.9\n",
      "Episode 679 reward: 21.6\n",
      "Episode 689 reward: 18.4\n",
      "Episode 699 reward: 19.1\n",
      "Episode 709 reward: 12.9\n",
      "Episode 719 reward: 16.7\n",
      "Episode 729 reward: 17.1\n",
      "Episode 739 reward: 17.4\n",
      "Episode 749 reward: 19.3\n",
      "Episode 759 reward: 19.4\n",
      "Episode 769 reward: 18.4\n",
      "Episode 779 reward: 21.7\n",
      "Episode 789 reward: 19.8\n",
      "Episode 799 reward: 15.2\n",
      "Episode 809 reward: 22.1\n",
      "Episode 819 reward: 20.2\n",
      "Episode 829 reward: 20.4\n",
      "Episode 839 reward: 21.4\n",
      "Episode 849 reward: 21.6\n",
      "Episode 859 reward: 20.5\n",
      "Episode 869 reward: 21.1\n",
      "Episode 879 reward: 22.2\n",
      "Episode 889 reward: 20.9\n",
      "Episode 899 reward: 20.1\n",
      "Episode 909 reward: 18.1\n",
      "Episode 919 reward: 19.8\n",
      "Episode 929 reward: 21.9\n",
      "Episode 939 reward: 18.8\n",
      "Episode 949 reward: 21.9\n",
      "Episode 959 reward: 22.0\n",
      "Episode 969 reward: 20.4\n",
      "Episode 979 reward: 20.1\n",
      "Episode 989 reward: 21.5\n",
      "Episode 999 reward: 22.2\n",
      "Saved Model\n",
      "Episode 1009 reward: 23.4\n",
      "Episode 1019 reward: 20.7\n",
      "Episode 1029 reward: 22.0\n",
      "Episode 1039 reward: 20.5\n",
      "Episode 1049 reward: 23.2\n",
      "Episode 1059 reward: 19.2\n",
      "Episode 1069 reward: 21.6\n",
      "Episode 1079 reward: 22.4\n",
      "Episode 1089 reward: 21.4\n",
      "Episode 1099 reward: 22.0\n",
      "Episode 1109 reward: 20.5\n",
      "Episode 1119 reward: 23.4\n",
      "Episode 1129 reward: 22.0\n",
      "Episode 1139 reward: 22.5\n",
      "Episode 1149 reward: 21.4\n",
      "Episode 1159 reward: 22.4\n",
      "Episode 1169 reward: 21.3\n",
      "Episode 1179 reward: 24.8\n",
      "Episode 1189 reward: 22.7\n",
      "Episode 1199 reward: 22.1\n",
      "Episode 1209 reward: 20.4\n",
      "Episode 1219 reward: 22.2\n",
      "Episode 1229 reward: 22.5\n",
      "Episode 1239 reward: 19.8\n",
      "Episode 1249 reward: 20.7\n",
      "Episode 1259 reward: 20.1\n",
      "Episode 1269 reward: 22.9\n",
      "Episode 1279 reward: 23.6\n",
      "Episode 1289 reward: 20.9\n",
      "Episode 1299 reward: 20.9\n",
      "Episode 1309 reward: 24.0\n",
      "Episode 1319 reward: 20.6\n",
      "Episode 1329 reward: 23.3\n",
      "Episode 1339 reward: 21.4\n",
      "Episode 1349 reward: 20.3\n",
      "Episode 1359 reward: 20.1\n",
      "Episode 1369 reward: 22.0\n",
      "Episode 1379 reward: 20.5\n",
      "Episode 1389 reward: 20.4\n",
      "Episode 1399 reward: 21.5\n",
      "Episode 1409 reward: 22.0\n",
      "Episode 1419 reward: 22.0\n",
      "Episode 1429 reward: 22.2\n",
      "Episode 1439 reward: 21.8\n",
      "Episode 1449 reward: 22.3\n",
      "Episode 1459 reward: 22.5\n",
      "Episode 1469 reward: 21.0\n",
      "Episode 1479 reward: 19.2\n",
      "Episode 1489 reward: 19.9\n",
      "Episode 1499 reward: 21.7\n",
      "Episode 1509 reward: 20.5\n",
      "Episode 1519 reward: 22.3\n",
      "Episode 1529 reward: 21.8\n",
      "Episode 1539 reward: 22.1\n",
      "Episode 1549 reward: 22.8\n",
      "Episode 1559 reward: 22.4\n",
      "Episode 1569 reward: 22.6\n",
      "Episode 1579 reward: 23.0\n",
      "Episode 1589 reward: 21.3\n",
      "Episode 1599 reward: 21.4\n",
      "Episode 1609 reward: 20.9\n",
      "Episode 1619 reward: 20.7\n",
      "Episode 1629 reward: 23.2\n",
      "Episode 1639 reward: 21.7\n",
      "Episode 1649 reward: 23.2\n",
      "Episode 1659 reward: 21.8\n",
      "Episode 1669 reward: 20.9\n",
      "Episode 1679 reward: 21.7\n",
      "Episode 1689 reward: 20.7\n",
      "Episode 1699 reward: 22.7\n",
      "Episode 1709 reward: 20.7\n",
      "Episode 1719 reward: 24.2\n",
      "Episode 1729 reward: 21.1\n",
      "Episode 1739 reward: 22.9\n",
      "Episode 1749 reward: 21.5\n",
      "Episode 1759 reward: 21.3\n",
      "Episode 1769 reward: 19.3\n",
      "Episode 1779 reward: 20.9\n",
      "Episode 1789 reward: 21.5\n",
      "Episode 1799 reward: 23.7\n",
      "Episode 1809 reward: 19.1\n",
      "Episode 1819 reward: 23.4\n",
      "Episode 1829 reward: 21.0\n",
      "Episode 1839 reward: 21.2\n",
      "Episode 1849 reward: 21.5\n",
      "Episode 1859 reward: 21.9\n",
      "Episode 1869 reward: 20.4\n",
      "Episode 1879 reward: 22.7\n",
      "Episode 1889 reward: 21.9\n",
      "Episode 1899 reward: 17.0\n",
      "Episode 1909 reward: 20.4\n",
      "Episode 1919 reward: 22.0\n",
      "Episode 1929 reward: 21.5\n",
      "Episode 1939 reward: 23.5\n",
      "Episode 1949 reward: 20.9\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-439bd54f2ea9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    104\u001b[0m                     \u001b[1;31m# (3) Update the primary network with our target values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m                     \u001b[1;31m#update use \"a\" for Q(s,a) not chosen_actions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m                     \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmainQN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdateModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mmainQN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscalarInput\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmainQN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtargetQ\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mQ_target\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmainQN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m                     \u001b[1;31m################################################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mT:\\xhuangat\\IDE\\python-3.5.3.amd64\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 778\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mT:\\xhuangat\\IDE\\python-3.5.3.amd64\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    980\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 982\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mT:\\xhuangat\\IDE\\python-3.5.3.amd64\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1032\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mT:\\xhuangat\\IDE\\python-3.5.3.amd64\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1037\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mT:\\xhuangat\\IDE\\python-3.5.3.amd64\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "mainQN = Qnetwork(h_size)\n",
    "targetQN = Qnetwork(h_size)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "trainables = tf.trainable_variables()\n",
    "\n",
    "targetOps = updateTargetGraph(trainables,tau)\n",
    "\n",
    "myBuffer = experience_buffer()\n",
    "\n",
    "#Set the rate of random action decrease. \n",
    "e = startE\n",
    "stepDrop = (startE - endE)/annealing_steps\n",
    "\n",
    "#create lists to contain total rewards and steps per episode\n",
    "jList = []\n",
    "rList = []\n",
    "total_steps = 0\n",
    "\n",
    "#Make a path for our model to be saved in.\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    if load_model == True:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(path)\n",
    "        saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    for i in range(num_episodes):\n",
    "        episodeBuffer = experience_buffer()\n",
    "        #Reset environment and get first new observation\n",
    "        s = env.reset()\n",
    "        s = processState(s)\n",
    "        d = False\n",
    "        rAll = 0\n",
    "        j = 0\n",
    "        #The Q-Network\n",
    "        while j < max_epLength: #If the agent takes longer than 50 moves to reach either of the blocks, end the trial.\n",
    "            j+=1\n",
    "            #Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "            if np.random.rand(1) < e or total_steps < pre_train_steps:#make sure to warm up before following the policy\n",
    "                a = np.random.randint(0,4)\n",
    "            else:\n",
    "                a = sess.run(mainQN.predict,feed_dict={mainQN.scalarInput:[s]})[0]#follow mainQN to explore [s] because it needs [NONE, 21138]\n",
    "            total_steps += 1\n",
    "            \n",
    "            ################################################################################\n",
    "            # TODO: Save the experience to our episode buffer.                             #\n",
    "            # You will need to do the following:                                           #\n",
    "            # (1) Get new state s1 (resized), reward r and done d from a                   #\n",
    "            # (2) Add experience to episode buffer. Hint: experience includes              #\n",
    "            #     s, a, r, s1 and d.                                                       #\n",
    "            ################################################################################\n",
    "            s1, r, d = env.step(a)\n",
    "            s1 = processState(s1)\n",
    "            experience = np.reshape(np.array([s, a , r, s1, d]),[1,5])# array can store different size matrix\n",
    "            episodeBuffer.add(experience)          \n",
    "            ################################################################################\n",
    "            #                                 END OF YOUR CODE                             #\n",
    "            ################################################################################\n",
    "            \n",
    "            if total_steps > pre_train_steps:\n",
    "                if e > endE:\n",
    "                    e -= stepDrop\n",
    "                \n",
    "                if total_steps % (update_freq) == 0:\n",
    "                    \n",
    "                    ################################################################################\n",
    "                    # TODO: Implement Double-DQN                                                   #\n",
    "                    # (1) Get a random batch of experiences via experience_buffer class            #\n",
    "                    #                                                                              #\n",
    "                    # (2) Perform the Double-DQN update to the target Q-values                     #\n",
    "                    #     Hint: Use mainQN and targetQN separately to chose an action and predict  #\n",
    "                    #     the Q-values for that action.                                            #\n",
    "                    #     Then compute targetQ based on Double-DQN equation                        #\n",
    "                    #                                                                              #\n",
    "                    # (3) Update the primary network with our target values                        #\n",
    "                    ################################################################################ \n",
    "                    # (1) Get a random batch of experiences via experience_buffer class\n",
    "                    batch = myBuffer.sample(batch_size)\n",
    "                    #a = batch[:,1]\n",
    "                    #r = batch[:,2]\n",
    "                    #d\n",
    "                    # (2) Perform the Double-DQN update to the target Q-values\n",
    "                    chosen_actions  = sess.run(mainQN.predict,feed_dict={mainQN.scalarInput: np.vstack(batch[:,3])}) # N,21138\n",
    "                    #Nx1\n",
    "                    #next_Q = sess.run(mainQN.predictQ, feed_dict = {targetQN.scalarInput:s1, \n",
    "                    #                                                 targetQN.actions: chosen_actions})# only feed in what it needs\n",
    "                                                                                                        # so no need to feed in targetQ         \n",
    "                    #because you cannot skip targetQ and only call the variable below it like above\n",
    "                    #but we dont know targetQ yet\n",
    "                    #so the only way is just make use of the value calculated before it Qout-> to find \"next_Q\"\n",
    "                    all_next_Q = sess.run(targetQN.Qout, feed_dict = {targetQN.scalarInput:np.vstack(batch[:,3])})#Nx4\n",
    "                    next_Q = all_next_Q[range(batch_size), chosen_actions]#only get the specific action Q, so Nx1\n",
    "                    \n",
    "                    \n",
    "                    end_multiplier = 1 - batch[:,4] \n",
    "                    Q_target = batch[:,2] + y*next_Q*end_multiplier #Nx1\n",
    "                    # (3) Update the primary network with our target values  \n",
    "                    #update use \"a\" for Q(s,a) not chosen_actions\n",
    "                    _ = sess.run(mainQN.updateModel, feed_dict = {mainQN.scalarInput:np.vstack(batch[:,0]), mainQN.targetQ: Q_target, mainQN.actions: batch[:,1]})\n",
    "                    \n",
    "                    ################################################################################\n",
    "                    #                                 END OF YOUR CODE                             #\n",
    "                    ################################################################################\n",
    "                           \n",
    "                    updateTarget(targetOps,sess) #Update the target network toward the primary network.\n",
    "            rAll += r\n",
    "            s = s1\n",
    "            \n",
    "            if d == True:\n",
    "\n",
    "                break\n",
    "        \n",
    "        myBuffer.add(episodeBuffer.buffer)\n",
    "        jList.append(j)\n",
    "        rList.append(rAll)\n",
    "        #Periodically save the model. \n",
    "        if i % 1000 == 0:\n",
    "            saver.save(sess,path+'/model-'+str(i)+'.ckpt')\n",
    "            print(\"Saved Model\")\n",
    "        if len(rList) % 10 == 0:\n",
    "            print(\"Episode\",i,\"reward:\",np.mean(rList[-10:]))\n",
    "    saver.save(sess,path+'/model-'+str(i)+'.ckpt')\n",
    "print(\"Mean reward per episode: \" + str(sum(rList)/num_episodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes about 40 minutes to train 5000 episodes in Lab 4 machines. Mean reward per episode (50 steps) should be around 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking network learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean reward over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1c0255af748>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c025554978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rMat = np.resize(np.array(rList),[len(rList)//100,100])\n",
    "rMean = np.average(rMat,1)\n",
    "plt.plot(rMean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Question:\n",
    "Try a basic DQN without Dueling DQN and Double DQN (i.e. only one single network, no decomposition of the Q-function). You don't need to provide detailed source, just some quantitative comparison is OK (e.g. by comparing the mean reward). **\n",
    "\n",
    "**Your answer:** *The source codes are shown below, and after 2000 epidsodes training, the mean reward of Basic DQN is still around 2. While as shown in above plot, after 2000 episodes, the mean reward of Double DQN has reached and stayed in the level of around 20. Therefore, Double DQN and Duel DQN perform around 10 times better than basic DQN*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Basic DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BasicQnetwork():\n",
    "    def __init__(self,h_size):\n",
    "        #The network recieves a frame from the game, flattened into an array.\n",
    "        #It then resizes it and processes it through four convolutional layers.\n",
    "        #We use slim.conv2d to set up our network \n",
    "        self.scalarInput =  tf.placeholder(shape=[None,21168],dtype=tf.float32)\n",
    "        self.imageIn = tf.reshape(self.scalarInput,shape=[-1,84,84,3])\n",
    "        self.conv1 = slim.conv2d( \\\n",
    "            inputs=self.imageIn,num_outputs=32,kernel_size=[8,8],stride=[4,4],padding='VALID', biases_initializer=None)\n",
    "        self.conv2 = slim.conv2d( \\\n",
    "            inputs=self.conv1,num_outputs=64,kernel_size=[4,4],stride=[2,2],padding='VALID', biases_initializer=None)\n",
    "        self.conv3 = slim.conv2d( \\\n",
    "            inputs=self.conv2,num_outputs=64,kernel_size=[3,3],stride=[1,1],padding='VALID', biases_initializer=None)\n",
    "        self.conv4 = slim.conv2d( \\\n",
    "            inputs=self.conv3,num_outputs=h_size,kernel_size=[7,7],stride=[1,1],padding='VALID', biases_initializer=None)\n",
    "        #self.conv4 size is Nx1x1xh_size\n",
    "        self.flat = slim.flatten(self.conv4)#Nxh_size\n",
    "        self.flatSize = self.flat.get_shape().as_list()#N x  ...\n",
    "        \n",
    "        initializer = tf.contrib.layers.xavier_initializer()\n",
    "        self.W = tf.Variable(initializer([self.flatSize[1],env.actions]))# ... x 4\n",
    "        \n",
    "        self.Qout = tf.matmul(self.flat, self.W)#Nx4 -> used for predictQ\n",
    "        self.predict = tf.argmax(self.Qout,1) #Nx1 -> only used for calculating targetQ \n",
    "        \n",
    "        #Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.\n",
    "        self.targetQ = tf.placeholder(shape=[None],dtype=tf.float32)#Nx1 value for Q*(s,a) by bellman\n",
    "        self.actions = tf.placeholder(shape=[None],dtype=tf.int32)#Nx1\n",
    "        self.actions_onehot = tf.one_hot(self.actions,env.actions,dtype=tf.float32)#Nx4\n",
    "        ################################################################################\n",
    "        # TODO:                                                                        #\n",
    "        # Obtain the loss (self.loss) by taking the sum of squares difference          #\n",
    "        # between the target and prediction Q values.                                  #\n",
    "        ################################################################################\n",
    "        #Qout is different from predictQ, Qout is Nx4, contain all actions' Q-value for every batch N\n",
    "        #while predictQ only picks the predicted Q-value for specific action because we only focus on Q(s,a)\n",
    "        self.predictQ = tf.reduce_sum(tf.multiply(self.Qout, self.actions_onehot),axis = 1)#Nx1 for Q*(s,a,theta) by NN\n",
    "        square_diff = tf.square(self.predictQ - self.targetQ)\n",
    "        self.loss = tf.reduce_mean(square_diff)\n",
    "        ################################################################################\n",
    "        #                                 END OF YOUR CODE                             #\n",
    "        ################################################################################\n",
    "        \n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        self.updateModel = self.trainer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Training the network DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32 #How many experiences to use for each training step.\n",
    "update_freq = 4 #How often to perform a training step.\n",
    "y = .99 #Discount factor on the target Q-values\n",
    "startE = 1 #Starting chance of random action\n",
    "endE = 0.1 #Final chance of random action\n",
    "annealing_steps = 10000. #How many steps of training to reduce startE to endE.\n",
    "num_episodes = 2000 #How many episodes of game environment to train network with.\n",
    "pre_train_steps = 10000 #How many steps of random actions before training begins.\n",
    "max_epLength = 50 #The max allowed length of our episode.\n",
    "load_model = False #Whether to load a saved model.\n",
    "path = \"./dqn\" #The path to save our model to.\n",
    "h_size = 512 #The size of the final convolutional layer before splitting it into Advantage and Value streams.\n",
    "tau = 0.001 #Rate to update target network toward primary network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model\n",
      "Episode 9 reward: 1.2\n",
      "Episode 19 reward: 1.7\n",
      "Episode 29 reward: 2.7\n",
      "Episode 39 reward: 1.7\n",
      "Episode 49 reward: 3.5\n",
      "Episode 59 reward: 0.6\n",
      "Episode 69 reward: 1.1\n",
      "Episode 79 reward: 3.1\n",
      "Episode 89 reward: 2.1\n",
      "Episode 99 reward: 2.7\n",
      "Episode 109 reward: 2.6\n",
      "Episode 119 reward: 2.1\n",
      "Episode 129 reward: 1.1\n",
      "Episode 139 reward: 1.9\n",
      "Episode 149 reward: 0.0\n",
      "Episode 159 reward: 0.8\n",
      "Episode 169 reward: 4.5\n",
      "Episode 179 reward: 2.2\n",
      "Episode 189 reward: 3.5\n",
      "Episode 199 reward: 2.8\n",
      "Episode 209 reward: 2.8\n",
      "Episode 219 reward: 2.7\n",
      "Episode 229 reward: 0.6\n",
      "Episode 239 reward: 1.3\n",
      "Episode 249 reward: 1.5\n",
      "Episode 259 reward: 2.0\n",
      "Episode 269 reward: 1.6\n",
      "Episode 279 reward: 2.7\n",
      "Episode 289 reward: 2.8\n",
      "Episode 299 reward: 1.4\n",
      "Episode 309 reward: 1.9\n",
      "Episode 319 reward: 2.2\n",
      "Episode 329 reward: 1.1\n",
      "Episode 339 reward: 0.9\n",
      "Episode 349 reward: 1.7\n",
      "Episode 359 reward: 0.6\n",
      "Episode 369 reward: 0.8\n",
      "Episode 379 reward: 2.0\n",
      "Episode 389 reward: 0.5\n",
      "Episode 399 reward: 0.7\n",
      "Episode 409 reward: 1.4\n",
      "Episode 419 reward: 1.5\n",
      "Episode 429 reward: 1.4\n",
      "Episode 439 reward: 1.0\n",
      "Episode 449 reward: 1.9\n",
      "Episode 459 reward: 2.2\n",
      "Episode 469 reward: 1.0\n",
      "Episode 479 reward: 1.4\n",
      "Episode 489 reward: 1.4\n",
      "Episode 499 reward: 2.0\n",
      "Episode 509 reward: 3.7\n",
      "Episode 519 reward: 1.0\n",
      "Episode 529 reward: 3.0\n",
      "Episode 539 reward: 1.9\n",
      "Episode 549 reward: 2.4\n",
      "Episode 559 reward: 2.1\n",
      "Episode 569 reward: 3.8\n",
      "Episode 579 reward: 1.7\n",
      "Episode 589 reward: 1.7\n",
      "Episode 599 reward: 4.2\n",
      "Episode 609 reward: 1.9\n",
      "Episode 619 reward: 2.0\n",
      "Episode 629 reward: 0.6\n",
      "Episode 639 reward: 1.3\n",
      "Episode 649 reward: 2.4\n",
      "Episode 659 reward: 2.6\n",
      "Episode 669 reward: 3.0\n",
      "Episode 679 reward: 2.1\n",
      "Episode 689 reward: 2.4\n",
      "Episode 699 reward: 0.3\n",
      "Episode 709 reward: 0.2\n",
      "Episode 719 reward: -0.3\n",
      "Episode 729 reward: 0.4\n",
      "Episode 739 reward: 0.9\n",
      "Episode 749 reward: -0.2\n",
      "Episode 759 reward: 0.7\n",
      "Episode 769 reward: 0.4\n",
      "Episode 779 reward: -0.1\n",
      "Episode 789 reward: 0.4\n",
      "Episode 799 reward: 0.1\n",
      "Episode 809 reward: 0.3\n",
      "Episode 819 reward: 0.8\n",
      "Episode 829 reward: 0.1\n",
      "Episode 839 reward: 0.3\n",
      "Episode 849 reward: 0.3\n",
      "Episode 859 reward: 0.7\n",
      "Episode 869 reward: 0.7\n",
      "Episode 879 reward: 1.4\n",
      "Episode 889 reward: 1.1\n",
      "Episode 899 reward: 0.8\n",
      "Episode 909 reward: 0.7\n",
      "Episode 919 reward: 1.3\n",
      "Episode 929 reward: 0.2\n",
      "Episode 939 reward: 0.5\n",
      "Episode 949 reward: 0.5\n",
      "Episode 959 reward: 1.3\n",
      "Episode 969 reward: 0.4\n",
      "Episode 979 reward: 0.9\n",
      "Episode 989 reward: 0.6\n",
      "Episode 999 reward: 1.1\n",
      "Saved Model\n",
      "Episode 1009 reward: 0.9\n",
      "Episode 1019 reward: 1.9\n",
      "Episode 1029 reward: 1.7\n",
      "Episode 1039 reward: 0.4\n",
      "Episode 1049 reward: 0.7\n",
      "Episode 1059 reward: 0.9\n",
      "Episode 1069 reward: 1.3\n",
      "Episode 1079 reward: 0.5\n",
      "Episode 1089 reward: 0.7\n",
      "Episode 1099 reward: 0.5\n",
      "Episode 1109 reward: 0.8\n",
      "Episode 1119 reward: 0.4\n",
      "Episode 1129 reward: 0.7\n",
      "Episode 1139 reward: 1.1\n",
      "Episode 1149 reward: 0.7\n",
      "Episode 1159 reward: 0.7\n",
      "Episode 1169 reward: 1.2\n",
      "Episode 1179 reward: 1.2\n",
      "Episode 1189 reward: 0.2\n",
      "Episode 1199 reward: 0.9\n",
      "Episode 1209 reward: 1.7\n",
      "Episode 1219 reward: 0.5\n",
      "Episode 1229 reward: 0.5\n",
      "Episode 1239 reward: 2.1\n",
      "Episode 1249 reward: -0.2\n",
      "Episode 1259 reward: 0.7\n",
      "Episode 1269 reward: 0.9\n",
      "Episode 1279 reward: 0.4\n",
      "Episode 1289 reward: 1.0\n",
      "Episode 1299 reward: 0.2\n",
      "Episode 1309 reward: 1.8\n",
      "Episode 1319 reward: 0.8\n",
      "Episode 1329 reward: 1.5\n",
      "Episode 1339 reward: 1.2\n",
      "Episode 1349 reward: 2.3\n",
      "Episode 1359 reward: 2.3\n",
      "Episode 1369 reward: 1.3\n",
      "Episode 1379 reward: 1.5\n",
      "Episode 1389 reward: 2.1\n",
      "Episode 1399 reward: 2.1\n",
      "Episode 1409 reward: 0.8\n",
      "Episode 1419 reward: 1.6\n",
      "Episode 1429 reward: 1.4\n",
      "Episode 1439 reward: 0.6\n",
      "Episode 1449 reward: 1.1\n",
      "Episode 1459 reward: 1.4\n",
      "Episode 1469 reward: 1.6\n",
      "Episode 1479 reward: 2.1\n",
      "Episode 1489 reward: 0.9\n",
      "Episode 1499 reward: 1.2\n",
      "Episode 1509 reward: 1.5\n",
      "Episode 1519 reward: 1.0\n",
      "Episode 1529 reward: 0.9\n",
      "Episode 1539 reward: 1.5\n",
      "Episode 1549 reward: 1.6\n",
      "Episode 1559 reward: 0.5\n",
      "Episode 1569 reward: 1.6\n",
      "Episode 1579 reward: 1.7\n",
      "Episode 1589 reward: 0.8\n",
      "Episode 1599 reward: 1.2\n",
      "Episode 1609 reward: 0.8\n",
      "Episode 1619 reward: 0.9\n",
      "Episode 1629 reward: 1.8\n",
      "Episode 1639 reward: 0.6\n",
      "Episode 1649 reward: 0.0\n",
      "Episode 1659 reward: 0.4\n",
      "Episode 1669 reward: 1.4\n",
      "Episode 1679 reward: 2.2\n",
      "Episode 1689 reward: 0.9\n",
      "Episode 1699 reward: 0.6\n",
      "Episode 1709 reward: 2.0\n",
      "Episode 1719 reward: 0.6\n",
      "Episode 1729 reward: 0.4\n",
      "Episode 1739 reward: 2.0\n",
      "Episode 1749 reward: 0.8\n",
      "Episode 1759 reward: 1.5\n",
      "Episode 1769 reward: 1.7\n",
      "Episode 1779 reward: 0.7\n",
      "Episode 1789 reward: 0.5\n",
      "Episode 1799 reward: -0.3\n",
      "Episode 1809 reward: 0.7\n",
      "Episode 1819 reward: 0.5\n",
      "Episode 1829 reward: 0.6\n",
      "Episode 1839 reward: 0.3\n",
      "Episode 1849 reward: 0.4\n",
      "Episode 1859 reward: 0.3\n",
      "Episode 1869 reward: 0.0\n",
      "Episode 1879 reward: 1.9\n",
      "Episode 1889 reward: 0.4\n",
      "Episode 1899 reward: 0.3\n",
      "Episode 1909 reward: 1.4\n",
      "Episode 1919 reward: 1.4\n",
      "Episode 1929 reward: 0.9\n",
      "Episode 1939 reward: 0.8\n",
      "Episode 1949 reward: 1.1\n",
      "Episode 1959 reward: 1.5\n",
      "Episode 1969 reward: 1.8\n",
      "Episode 1979 reward: 1.3\n",
      "Episode 1989 reward: 0.5\n",
      "Episode 1999 reward: 2.0\n",
      "Saved Model\n",
      "Episode 2009 reward: 1.2\n",
      "Episode 2019 reward: 2.1\n",
      "Episode 2029 reward: 0.4\n",
      "Episode 2039 reward: 1.5\n",
      "Episode 2049 reward: 0.4\n",
      "Episode 2059 reward: 1.1\n",
      "Episode 2069 reward: 0.7\n",
      "Episode 2079 reward: 1.2\n",
      "Episode 2089 reward: -0.1\n",
      "Episode 2099 reward: 2.2\n",
      "Episode 2109 reward: 0.3\n",
      "Episode 2119 reward: 1.6\n",
      "Episode 2129 reward: 1.1\n",
      "Episode 2139 reward: 0.2\n",
      "Episode 2149 reward: 1.1\n",
      "Episode 2159 reward: 0.5\n",
      "Episode 2169 reward: 0.3\n",
      "Episode 2179 reward: 0.7\n",
      "Episode 2189 reward: 1.8\n",
      "Episode 2199 reward: 1.9\n",
      "Episode 2209 reward: 0.4\n",
      "Episode 2219 reward: 0.7\n",
      "Episode 2229 reward: 0.9\n",
      "Episode 2239 reward: 0.4\n",
      "Episode 2249 reward: 0.8\n",
      "Episode 2259 reward: 0.5\n",
      "Episode 2269 reward: 0.1\n",
      "Episode 2279 reward: 1.5\n",
      "Episode 2289 reward: 1.0\n",
      "Episode 2299 reward: 0.9\n",
      "Episode 2309 reward: 1.2\n",
      "Episode 2319 reward: 1.8\n",
      "Episode 2329 reward: 0.5\n",
      "Episode 2339 reward: 0.8\n",
      "Episode 2349 reward: 0.1\n",
      "Episode 2359 reward: 0.9\n",
      "Episode 2369 reward: 1.5\n",
      "Episode 2379 reward: 0.4\n",
      "Episode 2389 reward: 0.3\n",
      "Episode 2399 reward: 0.9\n",
      "Episode 2409 reward: 0.8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-5ef8f4d01707>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     96\u001b[0m                     \u001b[1;31m# (3) Update the primary network with our target values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m                     \u001b[1;31m#update use \"a\" for Q(s,a) not chosen_actions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m                     \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbasicQN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdateModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mbasicQN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscalarInput\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasicQN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtargetQ\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mQ_target\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasicQN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m                     \u001b[1;31m################################################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mT:\\xhuangat\\IDE\\python-3.5.3.amd64\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 778\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mT:\\xhuangat\\IDE\\python-3.5.3.amd64\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    980\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 982\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mT:\\xhuangat\\IDE\\python-3.5.3.amd64\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1032\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mT:\\xhuangat\\IDE\\python-3.5.3.amd64\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1037\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mT:\\xhuangat\\IDE\\python-3.5.3.amd64\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "basicQN = BasicQnetwork(h_size)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "trainables = tf.trainable_variables()\n",
    "\n",
    "#targetOps = updateTargetGraph(trainables,tau)\n",
    "\n",
    "myBuffer = experience_buffer()\n",
    "\n",
    "#Set the rate of random action decrease. \n",
    "e = startE\n",
    "stepDrop = (startE - endE)/annealing_steps\n",
    "\n",
    "#create lists to contain total rewards and steps per episode\n",
    "jList = []\n",
    "rList = []\n",
    "total_steps = 0\n",
    "\n",
    "#Make a path for our model to be saved in.\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    if load_model == True:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(path)\n",
    "        saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    for i in range(num_episodes):\n",
    "        episodeBuffer = experience_buffer()\n",
    "        #Reset environment and get first new observation\n",
    "        s = env.reset()\n",
    "        s = processState(s)\n",
    "        d = False\n",
    "        rAll = 0\n",
    "        j = 0\n",
    "        #The Q-Network\n",
    "        while j < max_epLength: #If the agent takes longer than 50 moves to reach either of the blocks, end the trial.\n",
    "            j+=1\n",
    "            #Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "            if np.random.rand(1) < e or total_steps < pre_train_steps:#make sure to warm up before following the policy\n",
    "                a = np.random.randint(0,4)\n",
    "            else:\n",
    "                #follow basicQN to explore [s] because it needs [NONE, 21138]\n",
    "                a = sess.run(basicQN.predict,feed_dict={basicQN.scalarInput:[s]})[0]\n",
    "            total_steps += 1\n",
    "            \n",
    "            ################################################################################\n",
    "            # TODO: Save the experience to our episode buffer.                             #\n",
    "            # You will need to do the following:                                           #\n",
    "            # (1) Get new state s1 (resized), reward r and done d from a                   #\n",
    "            # (2) Add experience to episode buffer. Hint: experience includes              #\n",
    "            #     s, a, r, s1 and d.                                                       #\n",
    "            ################################################################################\n",
    "            s1, r, d = env.step(a)\n",
    "            s1 = processState(s1)\n",
    "            experience = np.reshape(np.array([s, a , r, s1, d]),[1,5])# array can store different size matrix\n",
    "            episodeBuffer.add(experience)          \n",
    "            ################################################################################\n",
    "            #                                 END OF YOUR CODE                             #\n",
    "            ################################################################################\n",
    "            \n",
    "            if total_steps > pre_train_steps:\n",
    "                if e > endE:\n",
    "                    e -= stepDrop\n",
    "                \n",
    "                if total_steps % (update_freq) == 0:\n",
    "                    \n",
    "                    ################################################################################\n",
    "                    # TODO: Implement basic-DQN                                                   #\n",
    "                    # (1) Get a random batch of experiences via experience_buffer class            #\n",
    "                    #                                                                              #\n",
    "                    # (2) Perform the basic-DQN update to the target Q-values                     #\n",
    "                    #     Hint:                                                                   #\n",
    "                    #     Then compute targetQ based on basic-DQN equation                        #\n",
    "                    #                                                                              #\n",
    "                    # (3) Update the basic network with our target values                        #\n",
    "                    ################################################################################ \n",
    "                    # (1) Get a random batch of experiences via experience_buffer class\n",
    "                    batch = myBuffer.sample(batch_size)\n",
    "                    #a = batch[:,1]\n",
    "                    #r = batch[:,2]\n",
    "                    #d\n",
    "                    # (2) Perform the Double-DQN update to the target Q-values\n",
    "                    all_next_Q, chosen_actions  = sess.run([basicQN.Qout, basicQN.predict],feed_dict= \\\n",
    "                                                           {basicQN.scalarInput: np.vstack(batch[:,3])}) # Nx4\n",
    "\n",
    "                    #next_Q = all_next_Q[range(batch_size), chosen_actions]#only get the specific action Q, so Nx1\n",
    "                    maxQ_next = np.max(all_next_Q, axis = 1)#Nx1\n",
    "                    \n",
    "                    end_multiplier = 1 - batch[:,4] \n",
    "                    Q_target = batch[:,2] + y*maxQ_next*end_multiplier #Nx1\n",
    "                    # (3) Update the primary network with our target values  \n",
    "                    #update use \"a\" for Q(s,a) not chosen_actions\n",
    "                    _ = sess.run(basicQN.updateModel, feed_dict = {basicQN.scalarInput:np.vstack(batch[:,0]), basicQN.targetQ: Q_target, basicQN.actions: batch[:,1]})\n",
    "                    \n",
    "                    ################################################################################\n",
    "                    #                                 END OF YOUR CODE                             #\n",
    "                    ################################################################################\n",
    "\n",
    "            rAll += r\n",
    "            s = s1\n",
    "            \n",
    "            if d == True:\n",
    "\n",
    "                break\n",
    "        \n",
    "        myBuffer.add(episodeBuffer.buffer)\n",
    "        jList.append(j)\n",
    "        rList.append(rAll)\n",
    "        #Periodically save the model. \n",
    "        if i % 1000 == 0:\n",
    "            saver.save(sess,path+'/model-'+str(i)+'.ckpt')\n",
    "            print(\"Saved Model\")\n",
    "        if len(rList) % 10 == 0:\n",
    "            print(\"Episode\",i,\"reward:\",np.mean(rList[-10:]))\n",
    "    saver.save(sess,path+'/model-'+str(i)+'.ckpt')\n",
    "print(\"Mean reward per episode: \" + str(sum(rList)/num_episodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the result of training of basic DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1c0255dcc18>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njLEoxMmdMcaiECd3xhiLQv8fqiZkBb2tKksAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c071e225c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rMat = np.resize(np.array(rList),[len(rList)//100,100])\n",
    "rMean = np.average(rMat,1)\n",
    "plt.plot(rMean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
